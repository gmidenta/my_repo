
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    COMMAND     ‚îÇ         ARGS         ‚îÇ PROFILE  ‚îÇ    USER     ‚îÇ VERSION ‚îÇ      START TIME      ‚îÇ       END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start          ‚îÇ --driver=docker      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 30 Sep 25 13:56 CEST ‚îÇ 30 Sep 25 13:56 CEST ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 02 Oct 25 17:10 CEST ‚îÇ 02 Oct 25 17:10 CEST ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 04 Oct 25 14:34 CEST ‚îÇ                      ‚îÇ
‚îÇ update-context ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 04 Oct 25 14:36 CEST ‚îÇ                      ‚îÇ
‚îÇ update-context ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 04 Oct 25 14:37 CEST ‚îÇ                      ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 04 Oct 25 14:38 CEST ‚îÇ 04 Oct 25 14:38 CEST ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 07 Oct 25 12:56 CEST ‚îÇ 07 Oct 25 12:56 CEST ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 13 Oct 25 15:21 CEST ‚îÇ 13 Oct 25 15:23 CEST ‚îÇ
‚îÇ config         ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 14:46 CEST ‚îÇ 14 Oct 25 14:46 CEST ‚îÇ
‚îÇ ip             ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 14:47 CEST ‚îÇ 14 Oct 25 14:47 CEST ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 15:08 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 15:12 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:04 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:05 CEST ‚îÇ                      ‚îÇ
‚îÇ tunnel         ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:05 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:08 CEST ‚îÇ                      ‚îÇ
‚îÇ ip             ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:08 CEST ‚îÇ 14 Oct 25 16:08 CEST ‚îÇ
‚îÇ tunnel         ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:11 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 14 Oct 25 16:11 CEST ‚îÇ                      ‚îÇ
‚îÇ start          ‚îÇ                      ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 16 Oct 25 12:33 CEST ‚îÇ 16 Oct 25 12:33 CEST ‚îÇ
‚îÇ service        ‚îÇ myapp-service --url  ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 16 Oct 25 16:49 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ voting-service --url ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 20 Oct 25 15:35 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ voting-service --url ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 21 Oct 25 11:34 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ result-service --url ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 21 Oct 25 15:40 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ voting-service --url ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 21 Oct 25 15:42 CEST ‚îÇ                      ‚îÇ
‚îÇ service        ‚îÇ result-service --url ‚îÇ minikube ‚îÇ tayebgmiden ‚îÇ v1.37.0 ‚îÇ 21 Oct 25 15:45 CEST ‚îÇ                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Dernier d√©marrage <==
Log file created at: 2025/10/16 12:33:28
Running on machine: MacBook-Pro-de-Tayeb
Binary: Built with gc go1.24.6 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1016 12:33:28.073125    1633 out.go:360] Setting OutFile to fd 1 ...
I1016 12:33:28.073266    1633 out.go:413] isatty.IsTerminal(1) = true
I1016 12:33:28.073268    1633 out.go:374] Setting ErrFile to fd 2...
I1016 12:33:28.073270    1633 out.go:413] isatty.IsTerminal(2) = true
I1016 12:33:28.073403    1633 root.go:338] Updating PATH: /Users/tayebgmiden/.minikube/bin
W1016 12:33:28.073502    1633 root.go:314] Error reading config file at /Users/tayebgmiden/.minikube/config/config.json: open /Users/tayebgmiden/.minikube/config/config.json: no such file or directory
I1016 12:33:28.073943    1633 out.go:368] Setting JSON to false
I1016 12:33:28.096025    1633 start.go:130] hostinfo: {"hostname":"MacBook-Pro-de-Tayeb.local","uptime":1784,"bootTime":1760609024,"procs":630,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.6","kernelVersion":"24.6.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"2fc81074-0be8-50a2-9317-a9a902f52806"}
W1016 12:33:28.096091    1633 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1016 12:33:28.101777    1633 out.go:179] üòÑ  minikube v1.37.0 sur Darwin 15.6 (arm64)
I1016 12:33:28.108895    1633 notify.go:220] Checking for updates...
I1016 12:33:28.109340    1633 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1016 12:33:28.109455    1633 driver.go:421] Setting default libvirt URI to qemu:///system
I1016 12:33:28.229986    1633 docker.go:123] docker version: linux-28.3.2:Docker Desktop 4.43.2 (199162)
I1016 12:33:28.230183    1633 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1016 12:33:28.301315    1633 info.go:266] docker info: {ID:780fea94-c97b-430e-9226-2b9d84f687e4 Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:11 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:55 OomKillDisable:false NGoroutines:89 SystemTime:2025-10-16 10:33:28.29132963 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8217890816 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/tayebgmiden/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/tayebgmiden/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/Users/tayebgmiden/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/Users/tayebgmiden/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/Users/tayebgmiden/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/Users/tayebgmiden/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/Users/tayebgmiden/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/Users/tayebgmiden/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/Users/tayebgmiden/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/tayebgmiden/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/Users/tayebgmiden/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/Users/tayebgmiden/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/tayebgmiden/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I1016 12:33:28.305843    1633 out.go:179] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I1016 12:33:28.308673    1633 start.go:304] selected driver: docker
I1016 12:33:28.308684    1633 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4600 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1016 12:33:28.308717    1633 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1016 12:33:28.308812    1633 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1016 12:33:28.377989    1633 info.go:266] docker info: {ID:780fea94-c97b-430e-9226-2b9d84f687e4 Containers:6 ContainersRunning:0 ContainersPaused:0 ContainersStopped:6 Images:11 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:55 OomKillDisable:false NGoroutines:89 SystemTime:2025-10-16 10:33:28.367735463 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8217890816 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/tayebgmiden/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/tayebgmiden/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/Users/tayebgmiden/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/Users/tayebgmiden/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/Users/tayebgmiden/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/Users/tayebgmiden/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/Users/tayebgmiden/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/Users/tayebgmiden/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/Users/tayebgmiden/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/tayebgmiden/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/Users/tayebgmiden/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/Users/tayebgmiden/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/tayebgmiden/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I1016 12:33:28.378187    1633 cni.go:84] Creating CNI manager for ""
I1016 12:33:28.378246    1633 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1016 12:33:28.378276    1633 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4600 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1016 12:33:28.381771    1633 out.go:179] üëç  D√©marrage du n≈ìud "minikube" primary control-plane dans le cluster "minikube"
I1016 12:33:28.384752    1633 cache.go:123] Beginning downloading kic base image for docker with docker
I1016 12:33:28.387745    1633 out.go:179] üöú  Extraction de l'image de base v0.0.48...
I1016 12:33:28.391777    1633 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1016 12:33:28.391789    1633 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1016 12:33:28.391810    1633 preload.go:146] Found local preload: /Users/tayebgmiden/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4
I1016 12:33:28.391813    1633 cache.go:58] Caching tarball of preloaded images
I1016 12:33:28.392063    1633 preload.go:172] Found /Users/tayebgmiden/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I1016 12:33:28.392118    1633 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1016 12:33:28.392279    1633 profile.go:143] Saving config to /Users/tayebgmiden/.minikube/profiles/minikube/config.json ...
I1016 12:33:28.443078    1633 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1016 12:33:28.443280    1633 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1016 12:33:28.443316    1633 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I1016 12:33:28.443328    1633 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I1016 12:33:28.443331    1633 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1016 12:33:28.443333    1633 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1016 12:33:35.714273    1633 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1016 12:33:35.714332    1633 cache.go:232] Successfully downloaded all kic artifacts
I1016 12:33:35.714399    1633 start.go:360] acquireMachinesLock for minikube: {Name:mk7bb16fe0527106ab5597c3a1700a9dc7a6ba83 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1016 12:33:35.714661    1633 start.go:364] duration metric: took 238.084¬µs to acquireMachinesLock for "minikube"
I1016 12:33:35.714715    1633 start.go:96] Skipping create...Using existing machine configuration
I1016 12:33:35.714718    1633 fix.go:54] fixHost starting: 
I1016 12:33:35.715020    1633 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1016 12:33:35.736799    1633 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1016 12:33:35.736828    1633 fix.go:138] unexpected machine state, will restart: <nil>
I1016 12:33:35.738867    1633 out.go:252] üîÑ  Red√©marrage du docker container existant pour "minikube" ...
I1016 12:33:35.738997    1633 cli_runner.go:164] Run: docker start minikube
I1016 12:33:35.865713    1633 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1016 12:33:35.886129    1633 kic.go:430] container "minikube" state is running.
I1016 12:33:35.886502    1633 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1016 12:33:35.899217    1633 profile.go:143] Saving config to /Users/tayebgmiden/.minikube/profiles/minikube/config.json ...
I1016 12:33:35.899497    1633 machine.go:93] provisionDockerMachine start ...
I1016 12:33:35.899556    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:35.912556    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:35.912892    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:35.912897    1633 main.go:141] libmachine: About to run SSH command:
hostname
I1016 12:33:35.914273    1633 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1016 12:33:39.075250    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1016 12:33:39.075306    1633 ubuntu.go:182] provisioning hostname "minikube"
I1016 12:33:39.075465    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:39.107199    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:39.107487    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:39.107495    1633 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1016 12:33:39.258896    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1016 12:33:39.259078    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:39.291891    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:39.292128    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:39.292136    1633 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1016 12:33:39.427450    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1016 12:33:39.427492    1633 ubuntu.go:188] set auth options {CertDir:/Users/tayebgmiden/.minikube CaCertPath:/Users/tayebgmiden/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/tayebgmiden/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/tayebgmiden/.minikube/machines/server.pem ServerKeyPath:/Users/tayebgmiden/.minikube/machines/server-key.pem ClientKeyPath:/Users/tayebgmiden/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/tayebgmiden/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/tayebgmiden/.minikube}
I1016 12:33:39.427536    1633 ubuntu.go:190] setting up certificates
I1016 12:33:39.427557    1633 provision.go:84] configureAuth start
I1016 12:33:39.427820    1633 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1016 12:33:39.462258    1633 provision.go:143] copyHostCerts
I1016 12:33:39.462599    1633 exec_runner.go:144] found /Users/tayebgmiden/.minikube/ca.pem, removing ...
I1016 12:33:39.462635    1633 exec_runner.go:203] rm: /Users/tayebgmiden/.minikube/ca.pem
I1016 12:33:39.462831    1633 exec_runner.go:151] cp: /Users/tayebgmiden/.minikube/certs/ca.pem --> /Users/tayebgmiden/.minikube/ca.pem (1090 bytes)
I1016 12:33:39.464063    1633 exec_runner.go:144] found /Users/tayebgmiden/.minikube/cert.pem, removing ...
I1016 12:33:39.464065    1633 exec_runner.go:203] rm: /Users/tayebgmiden/.minikube/cert.pem
I1016 12:33:39.464147    1633 exec_runner.go:151] cp: /Users/tayebgmiden/.minikube/certs/cert.pem --> /Users/tayebgmiden/.minikube/cert.pem (1135 bytes)
I1016 12:33:39.464636    1633 exec_runner.go:144] found /Users/tayebgmiden/.minikube/key.pem, removing ...
I1016 12:33:39.464639    1633 exec_runner.go:203] rm: /Users/tayebgmiden/.minikube/key.pem
I1016 12:33:39.464703    1633 exec_runner.go:151] cp: /Users/tayebgmiden/.minikube/certs/key.pem --> /Users/tayebgmiden/.minikube/key.pem (1675 bytes)
I1016 12:33:39.465301    1633 provision.go:117] generating server cert: /Users/tayebgmiden/.minikube/machines/server.pem ca-key=/Users/tayebgmiden/.minikube/certs/ca.pem private-key=/Users/tayebgmiden/.minikube/certs/ca-key.pem org=tayebgmiden.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1016 12:33:39.716390    1633 provision.go:177] copyRemoteCerts
I1016 12:33:39.716693    1633 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1016 12:33:39.716731    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:39.734916    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:39.841755    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I1016 12:33:39.862593    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I1016 12:33:39.875359    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1016 12:33:39.885664    1633 provision.go:87] duration metric: took 458.094208ms to configureAuth
I1016 12:33:39.885672    1633 ubuntu.go:206] setting minikube options for container-runtime
I1016 12:33:39.885832    1633 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1016 12:33:39.885886    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:39.903864    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:39.904051    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:39.904055    1633 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1016 12:33:40.034387    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1016 12:33:40.034450    1633 ubuntu.go:71] root file system type: overlay
I1016 12:33:40.034560    1633 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1016 12:33:40.034704    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.065353    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:40.065612    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:40.065674    1633 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1016 12:33:40.211272    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1016 12:33:40.211421    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.242515    1633 main.go:141] libmachine: Using SSH client type: native
I1016 12:33:40.242837    1633 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104f737c0] 0x104f75f80 <nil>  [] 0s} 127.0.0.1 50818 <nil> <nil>}
I1016 12:33:40.242850    1633 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1016 12:33:40.386585    1633 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1016 12:33:40.386605    1633 machine.go:96] duration metric: took 4.48704425s to provisionDockerMachine
I1016 12:33:40.386619    1633 start.go:293] postStartSetup for "minikube" (driver="docker")
I1016 12:33:40.386650    1633 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1016 12:33:40.386914    1633 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1016 12:33:40.387083    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.419113    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:40.516367    1633 ssh_runner.go:195] Run: cat /etc/os-release
I1016 12:33:40.519756    1633 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1016 12:33:40.519824    1633 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1016 12:33:40.519835    1633 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1016 12:33:40.519844    1633 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1016 12:33:40.519857    1633 filesync.go:126] Scanning /Users/tayebgmiden/.minikube/addons for local assets ...
I1016 12:33:40.520187    1633 filesync.go:126] Scanning /Users/tayebgmiden/.minikube/files for local assets ...
I1016 12:33:40.520263    1633 start.go:296] duration metric: took 133.619ms for postStartSetup
I1016 12:33:40.520454    1633 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1016 12:33:40.520563    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.555585    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:40.649812    1633 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1016 12:33:40.653727    1633 fix.go:56] duration metric: took 4.938938375s for fixHost
I1016 12:33:40.653749    1633 start.go:83] releasing machines lock for "minikube", held for 4.9390195s
I1016 12:33:40.653986    1633 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1016 12:33:40.692465    1633 ssh_runner.go:195] Run: cat /version.json
I1016 12:33:40.692586    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.693065    1633 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1016 12:33:40.693592    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:40.716906    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:40.717250    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:40.933415    1633 ssh_runner.go:195] Run: systemctl --version
I1016 12:33:40.938498    1633 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1016 12:33:40.941519    1633 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1016 12:33:40.950762    1633 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1016 12:33:40.950921    1633 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1016 12:33:40.954839    1633 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1016 12:33:40.954859    1633 start.go:495] detecting cgroup driver to use...
I1016 12:33:40.954885    1633 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1016 12:33:40.955443    1633 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1016 12:33:40.961556    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1016 12:33:40.965569    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1016 12:33:40.969393    1633 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1016 12:33:40.969503    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1016 12:33:40.973101    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1016 12:33:40.976780    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1016 12:33:40.980393    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1016 12:33:40.984021    1633 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1016 12:33:40.987339    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1016 12:33:40.990974    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1016 12:33:40.994571    1633 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1016 12:33:40.998236    1633 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1016 12:33:41.003964    1633 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1016 12:33:41.007031    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:41.033604    1633 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1016 12:33:41.081422    1633 start.go:495] detecting cgroup driver to use...
I1016 12:33:41.081451    1633 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1016 12:33:41.081642    1633 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1016 12:33:41.086512    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1016 12:33:41.090974    1633 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1016 12:33:41.100189    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1016 12:33:41.104357    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1016 12:33:41.109894    1633 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1016 12:33:41.116091    1633 ssh_runner.go:195] Run: which cri-dockerd
I1016 12:33:41.117665    1633 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1016 12:33:41.120670    1633 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1016 12:33:41.126662    1633 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1016 12:33:41.153065    1633 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1016 12:33:41.179415    1633 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1016 12:33:41.179614    1633 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1016 12:33:41.185905    1633 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1016 12:33:41.190367    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:41.215843    1633 ssh_runner.go:195] Run: sudo systemctl restart docker
I1016 12:33:42.208246    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1016 12:33:42.213787    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1016 12:33:42.218638    1633 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1016 12:33:42.223698    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1016 12:33:42.228155    1633 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1016 12:33:42.255403    1633 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1016 12:33:42.281214    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:42.307805    1633 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1016 12:33:42.344222    1633 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1016 12:33:42.348462    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:42.375221    1633 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1016 12:33:42.509948    1633 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1016 12:33:42.514837    1633 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1016 12:33:42.515119    1633 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1016 12:33:42.516605    1633 start.go:563] Will wait 60s for crictl version
I1016 12:33:42.516703    1633 ssh_runner.go:195] Run: which crictl
I1016 12:33:42.518403    1633 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1016 12:33:42.577461    1633 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1016 12:33:42.577608    1633 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1016 12:33:42.627866    1633 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1016 12:33:42.666562    1633 out.go:252] üê≥  Pr√©paration de Kubernetes v1.34.0 sur Docker 28.4.0...
I1016 12:33:42.666754    1633 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1016 12:33:42.759096    1633 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1016 12:33:42.759608    1633 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1016 12:33:42.761707    1633 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1016 12:33:42.766192    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1016 12:33:42.791770    1633 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4600 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1016 12:33:42.791841    1633 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1016 12:33:42.791913    1633 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1016 12:33:42.802588    1633 docker.go:691] Got preloaded images: -- stdout --
nginx:latest
gmidenta/hello-web:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1016 12:33:42.802595    1633 docker.go:621] Images already preloaded, skipping extraction
I1016 12:33:42.802700    1633 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1016 12:33:42.812262    1633 docker.go:691] Got preloaded images: -- stdout --
nginx:latest
gmidenta/hello-web:latest
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1016 12:33:42.812272    1633 cache_images.go:85] Images are preloaded, skipping loading
I1016 12:33:42.812295    1633 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1016 12:33:42.812419    1633 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1016 12:33:42.812483    1633 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1016 12:33:42.917018    1633 cni.go:84] Creating CNI manager for ""
I1016 12:33:42.917033    1633 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1016 12:33:42.917042    1633 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1016 12:33:42.917057    1633 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1016 12:33:42.917162    1633 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1016 12:33:42.917395    1633 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1016 12:33:42.921622    1633 binaries.go:44] Found k8s binaries, skipping transfer
I1016 12:33:42.921770    1633 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1016 12:33:42.924947    1633 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1016 12:33:42.930831    1633 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1016 12:33:42.936577    1633 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1016 12:33:42.942905    1633 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1016 12:33:42.944694    1633 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1016 12:33:42.948621    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:42.974713    1633 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1016 12:33:42.997453    1633 certs.go:68] Setting up /Users/tayebgmiden/.minikube/profiles/minikube for IP: 192.168.49.2
I1016 12:33:42.997509    1633 certs.go:194] generating shared ca certs ...
I1016 12:33:42.997539    1633 certs.go:226] acquiring lock for ca certs: {Name:mk87c1a4037a4515feea065f981da75cac060147 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1016 12:33:42.998755    1633 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/tayebgmiden/.minikube/ca.key
I1016 12:33:42.999174    1633 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/tayebgmiden/.minikube/proxy-client-ca.key
I1016 12:33:42.999183    1633 certs.go:256] generating profile certs ...
I1016 12:33:42.999545    1633 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/tayebgmiden/.minikube/profiles/minikube/client.key
I1016 12:33:43.000006    1633 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/tayebgmiden/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1016 12:33:43.000160    1633 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/tayebgmiden/.minikube/profiles/minikube/proxy-client.key
I1016 12:33:43.000442    1633 certs.go:484] found cert: /Users/tayebgmiden/.minikube/certs/ca-key.pem (1675 bytes)
I1016 12:33:43.000499    1633 certs.go:484] found cert: /Users/tayebgmiden/.minikube/certs/ca.pem (1090 bytes)
I1016 12:33:43.000541    1633 certs.go:484] found cert: /Users/tayebgmiden/.minikube/certs/cert.pem (1135 bytes)
I1016 12:33:43.000580    1633 certs.go:484] found cert: /Users/tayebgmiden/.minikube/certs/key.pem (1675 bytes)
I1016 12:33:43.001357    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1016 12:33:43.010225    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1016 12:33:43.018731    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1016 12:33:43.027638    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1016 12:33:43.036437    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1016 12:33:43.044909    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1016 12:33:43.053490    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1016 12:33:43.062064    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1016 12:33:43.070420    1633 ssh_runner.go:362] scp /Users/tayebgmiden/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1016 12:33:43.079020    1633 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I1016 12:33:43.087164    1633 ssh_runner.go:195] Run: openssl version
I1016 12:33:43.091757    1633 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1016 12:33:43.096101    1633 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1016 12:33:43.097703    1633 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep 30 11:56 /usr/share/ca-certificates/minikubeCA.pem
I1016 12:33:43.097773    1633 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1016 12:33:43.100789    1633 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1016 12:33:43.104169    1633 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1016 12:33:43.105663    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1016 12:33:43.108823    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1016 12:33:43.111827    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1016 12:33:43.114880    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1016 12:33:43.118280    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1016 12:33:43.121316    1633 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1016 12:33:43.123939    1633 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:4600 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1016 12:33:43.124082    1633 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1016 12:33:43.135457    1633 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1016 12:33:43.140547    1633 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1016 12:33:43.140579    1633 kubeadm.go:589] restartPrimaryControlPlane start ...
I1016 12:33:43.140706    1633 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1016 12:33:43.145514    1633 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1016 12:33:43.145630    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1016 12:33:43.168917    1633 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:53800"
I1016 12:33:43.168944    1633 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:53800, want: 127.0.0.1:50817
I1016 12:33:43.169118    1633 kubeconfig.go:62] /Users/tayebgmiden/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I1016 12:33:43.169276    1633 lock.go:35] WriteFile acquiring /Users/tayebgmiden/.kube/config: {Name:mkf88e37fd1b4a46c99b5ae24c62de2a116b955a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1016 12:33:43.170831    1633 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1016 12:33:43.174811    1633 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1016 12:33:43.174833    1633 kubeadm.go:593] duration metric: took 34.251458ms to restartPrimaryControlPlane
I1016 12:33:43.174837    1633 kubeadm.go:394] duration metric: took 50.903875ms to StartCluster
I1016 12:33:43.174848    1633 settings.go:142] acquiring lock: {Name:mk91fdf5a45f17b30058ee5414eee2ba3a1b47c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1016 12:33:43.175038    1633 settings.go:150] Updating kubeconfig:  /Users/tayebgmiden/.kube/config
I1016 12:33:43.175372    1633 lock.go:35] WriteFile acquiring /Users/tayebgmiden/.kube/config: {Name:mkf88e37fd1b4a46c99b5ae24c62de2a116b955a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1016 12:33:43.175519    1633 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1016 12:33:43.175531    1633 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1016 12:33:43.175588    1633 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1016 12:33:43.175606    1633 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W1016 12:33:43.175609    1633 addons.go:247] addon storage-provisioner should already be in state true
I1016 12:33:43.175607    1633 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1016 12:33:43.175623    1633 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1016 12:33:43.175630    1633 host.go:66] Checking if "minikube" exists ...
I1016 12:33:43.175655    1633 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1016 12:33:43.175878    1633 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1016 12:33:43.175912    1633 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1016 12:33:43.179790    1633 out.go:179] üîé  V√©rification des composants Kubernetes...
I1016 12:33:43.185606    1633 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1016 12:33:43.194382    1633 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1016 12:33:43.194387    1633 addons.go:247] addon default-storageclass should already be in state true
I1016 12:33:43.194398    1633 host.go:66] Checking if "minikube" exists ...
I1016 12:33:43.194598    1633 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1016 12:33:43.195022    1633 out.go:179]     ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I1016 12:33:43.198948    1633 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1016 12:33:43.198956    1633 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1016 12:33:43.199690    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:43.233316    1633 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1016 12:33:43.233347    1633 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1016 12:33:43.233495    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1016 12:33:43.236680    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:43.258403    1633 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50818 SSHKeyPath:/Users/tayebgmiden/.minikube/machines/minikube/id_rsa Username:docker}
I1016 12:33:43.273905    1633 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1016 12:33:43.286298    1633 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1016 12:33:43.301052    1633 api_server.go:52] waiting for apiserver process to appear ...
I1016 12:33:43.301141    1633 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1016 12:33:43.385419    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1016 12:33:43.403454    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W1016 12:33:43.492645    1633 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.492679    1633 retry.go:31] will retry after 266.663459ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1016 12:33:43.493823    1633 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.493844    1633 retry.go:31] will retry after 302.275544ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.760773    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1016 12:33:43.795237    1633 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.795261    1633 retry.go:31] will retry after 358.912392ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.796372    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1016 12:33:43.801472    1633 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1016 12:33:43.845887    1633 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:43.845911    1633 retry.go:31] will retry after 297.651668ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1016 12:33:44.144777    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1016 12:33:44.155375    1633 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1016 12:33:44.302278    1633 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1016 12:33:45.574750    1633 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.4298715s)
I1016 12:33:45.836461    1633 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.68102375s)
I1016 12:33:45.836516    1633 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.534196708s)
I1016 12:33:45.836537    1633 api_server.go:72] duration metric: took 2.660974209s to wait for apiserver process to appear ...
I1016 12:33:45.836549    1633 api_server.go:88] waiting for apiserver healthz status ...
I1016 12:33:45.836567    1633 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50817/healthz ...
I1016 12:33:45.841959    1633 api_server.go:279] https://127.0.0.1:50817/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1016 12:33:45.841980    1633 api_server.go:103] status: https://127.0.0.1:50817/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1016 12:33:45.851349    1633 out.go:179] üåü  Modules activ√©s: default-storageclass, storage-provisioner
I1016 12:33:45.854210    1633 addons.go:514] duration metric: took 2.678597917s for enable addons: enabled=[default-storageclass storage-provisioner]
I1016 12:33:46.336744    1633 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50817/healthz ...
I1016 12:33:46.341849    1633 api_server.go:279] https://127.0.0.1:50817/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1016 12:33:46.341861    1633 api_server.go:103] status: https://127.0.0.1:50817/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1016 12:33:46.837373    1633 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50817/healthz ...
I1016 12:33:46.844856    1633 api_server.go:279] https://127.0.0.1:50817/healthz returned 200:
ok
I1016 12:33:46.846913    1633 api_server.go:141] control plane version: v1.34.0
I1016 12:33:46.846930    1633 api_server.go:131] duration metric: took 1.010363959s to wait for apiserver health ...
I1016 12:33:46.846959    1633 system_pods.go:43] waiting for kube-system pods to appear ...
I1016 12:33:46.854315    1633 system_pods.go:59] 7 kube-system pods found
I1016 12:33:46.854337    1633 system_pods.go:61] "coredns-66bc5c9577-74j4j" [34282a99-0bdb-49bb-a6e7-14d2e2f1a1e5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1016 12:33:46.854349    1633 system_pods.go:61] "etcd-minikube" [5ad5d4c6-8be2-4370-a2c2-f2f69f6f3441] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1016 12:33:46.854355    1633 system_pods.go:61] "kube-apiserver-minikube" [ee2c5cb0-91a8-4c96-be54-4c0029186b73] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1016 12:33:46.854362    1633 system_pods.go:61] "kube-controller-manager-minikube" [6f55dd16-ba9c-4ead-8cc5-38b9b10c1412] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1016 12:33:46.854366    1633 system_pods.go:61] "kube-proxy-m7bh2" [78e9da66-d0d6-4eaf-8f13-55da2d772459] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1016 12:33:46.854370    1633 system_pods.go:61] "kube-scheduler-minikube" [bbc85f93-89f9-4fdd-8017-0649c5f18b2b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1016 12:33:46.854375    1633 system_pods.go:61] "storage-provisioner" [24c1b6d8-6b1a-4f68-b472-02f022b9a1f1] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1016 12:33:46.854380    1633 system_pods.go:74] duration metric: took 7.415875ms to wait for pod list to return data ...
I1016 12:33:46.854389    1633 kubeadm.go:578] duration metric: took 3.678815084s to wait for: map[apiserver:true system_pods:true]
I1016 12:33:46.854401    1633 node_conditions.go:102] verifying NodePressure condition ...
I1016 12:33:46.944471    1633 node_conditions.go:122] node storage ephemeral capacity is 474095688Ki
I1016 12:33:46.944489    1633 node_conditions.go:123] node cpu capacity is 11
I1016 12:33:46.944498    1633 node_conditions.go:105] duration metric: took 90.092917ms to run NodePressure ...
I1016 12:33:46.944505    1633 start.go:241] waiting for startup goroutines ...
I1016 12:33:46.944520    1633 start.go:246] waiting for cluster config update ...
I1016 12:33:46.944542    1633 start.go:255] writing updated cluster config ...
I1016 12:33:46.945071    1633 ssh_runner.go:195] Run: rm -f paused
I1016 12:33:47.084820    1633 start.go:617] kubectl: 1.34.1, cluster: 1.34.0 (minor skew: 0)
I1016 12:33:47.092937    1633 out.go:179] üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.


==> Docker <==
Oct 21 02:48:23 minikube cri-dockerd[1144]: time="2025-10-21T02:48:23Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 02:48:23 minikube dockerd[813]: time="2025-10-21T02:48:23.291672917Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 02:48:23 minikube dockerd[813]: time="2025-10-21T02:48:23.291693875Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 04:05:23 minikube dockerd[813]: time="2025-10-21T04:05:23.964039252Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/library/postgres/manifests/latest\": EOF"
Oct 21 04:05:23 minikube dockerd[813]: time="2025-10-21T04:05:23.966070127Z" level=error msg="Handler for POST /v1.46/images/create returned error: Head \"https://registry-1.docker.io/v2/library/postgres/manifests/latest\": EOF"
Oct 21 07:53:36 minikube cri-dockerd[1144]: time="2025-10-21T07:53:36Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 07:53:36 minikube dockerd[813]: time="2025-10-21T07:53:36.338843129Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 07:53:36 minikube dockerd[813]: time="2025-10-21T07:53:36.338848129Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 08:53:46 minikube cri-dockerd[1144]: time="2025-10-21T08:53:46Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 08:53:46 minikube dockerd[813]: time="2025-10-21T08:53:46.472697419Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 08:53:46 minikube dockerd[813]: time="2025-10-21T08:53:46.472718961Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 08:59:00 minikube cri-dockerd[1144]: time="2025-10-21T08:59:00Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 08:59:00 minikube dockerd[813]: time="2025-10-21T08:59:00.096549384Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 08:59:00 minikube dockerd[813]: time="2025-10-21T08:59:00.096560884Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:04:14 minikube cri-dockerd[1144]: time="2025-10-21T09:04:14Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:04:14 minikube dockerd[813]: time="2025-10-21T09:04:14.099362459Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:04:14 minikube dockerd[813]: time="2025-10-21T09:04:14.099372126Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:09:21 minikube cri-dockerd[1144]: time="2025-10-21T09:09:21Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:09:21 minikube dockerd[813]: time="2025-10-21T09:09:21.091458546Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:09:21 minikube dockerd[813]: time="2025-10-21T09:09:21.091475504Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:14:31 minikube cri-dockerd[1144]: time="2025-10-21T09:14:31Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:14:31 minikube dockerd[813]: time="2025-10-21T09:14:31.097437426Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:14:31 minikube dockerd[813]: time="2025-10-21T09:14:31.097542634Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:19:32 minikube cri-dockerd[1144]: time="2025-10-21T09:19:32Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:19:32 minikube dockerd[813]: time="2025-10-21T09:19:32.186911760Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:19:32 minikube dockerd[813]: time="2025-10-21T09:19:32.187044843Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:24:34 minikube cri-dockerd[1144]: time="2025-10-21T09:24:34Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:24:35 minikube dockerd[813]: time="2025-10-21T09:24:35.035106511Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:24:35 minikube dockerd[813]: time="2025-10-21T09:24:35.035108053Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:29:46 minikube cri-dockerd[1144]: time="2025-10-21T09:29:46Z" level=info msg="Stop pulling image postgres:latest: Status: Image is up to date for postgres:latest"
Oct 21 09:29:46 minikube dockerd[813]: time="2025-10-21T09:29:46.491178169Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stderr
Oct 21 09:29:46 minikube dockerd[813]: time="2025-10-21T09:29:46.491206919Z" level=error msg="copy stream failed" error="reading from a closed fifo" stream=stdout
Oct 21 09:31:59 minikube dockerd[813]: time="2025-10-21T09:31:59.488148092Z" level=info msg="ignoring event" container=bc853fbc062696771fd8f3abcee713e565c558f5212651aa021b282c112b1a15 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 09:32:18 minikube cri-dockerd[1144]: time="2025-10-21T09:32:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95ea8e8983bc192c43f5169c9f0d98ea2cb7a88fc4b2f1291d2b8c066195dc0f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 09:32:19 minikube dockerd[813]: time="2025-10-21T09:32:19.361817170Z" level=error msg="Not continuing with pull after error" error="manifest unknown: manifest unknown"
Oct 21 09:32:33 minikube dockerd[813]: time="2025-10-21T09:32:33.167147510Z" level=error msg="Not continuing with pull after error" error="manifest unknown: manifest unknown"
Oct 21 09:32:47 minikube dockerd[813]: time="2025-10-21T09:32:47.160335961Z" level=info msg="ignoring event" container=95ea8e8983bc192c43f5169c9f0d98ea2cb7a88fc4b2f1291d2b8c066195dc0f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 09:32:53 minikube cri-dockerd[1144]: time="2025-10-21T09:32:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60411b3f6c01ffa55253207df239f0004340cd6c426e0e8cf7eca4782a811ece/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 09:32:59 minikube cri-dockerd[1144]: time="2025-10-21T09:32:59Z" level=info msg="Stop pulling image postgres:15: Status: Downloaded newer image for postgres:15"
Oct 21 13:31:10 minikube cri-dockerd[1144]: time="2025-10-21T13:31:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ff0ef568007df731fc8b61b7a2186dba43a66cfec55de94418e7907b97569312/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 13:31:21 minikube cri-dockerd[1144]: time="2025-10-21T13:31:21Z" level=info msg="Pulling image kodekloud/examplevotingapp_worker:v1: b800e4c6f9e9: Extracting [=====================================>             ]  206.1MB/277MB"
Oct 21 13:31:23 minikube cri-dockerd[1144]: time="2025-10-21T13:31:23Z" level=info msg="Stop pulling image kodekloud/examplevotingapp_worker:v1: Status: Downloaded newer image for kodekloud/examplevotingapp_worker:v1"
Oct 21 13:31:24 minikube dockerd[813]: time="2025-10-21T13:31:24.502632090Z" level=info msg="ignoring event" container=b1d6e7a071f58f052a420b520ecc747c621f0302b3890048298da62dc9bc42ae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:31:25 minikube dockerd[813]: time="2025-10-21T13:31:25.101657007Z" level=info msg="ignoring event" container=1642cf4fd2eeebb6f891f35944c73c197da35d6fe4fa5517496cd427c5bebe98 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:31:41 minikube dockerd[813]: time="2025-10-21T13:31:41.361916251Z" level=info msg="ignoring event" container=abac4332b1e3ab8531c994ba1603f1932bf9590aae20d674045c806b2cde2edd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:32:09 minikube dockerd[813]: time="2025-10-21T13:32:09.269107667Z" level=info msg="ignoring event" container=e12776d368967865161917545975223aadfe8be2434f7faedfefd749f8b90fcf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:32:52 minikube dockerd[813]: time="2025-10-21T13:32:52.815515715Z" level=info msg="ignoring event" container=ff0ef568007df731fc8b61b7a2186dba43a66cfec55de94418e7907b97569312 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:33:25 minikube cri-dockerd[1144]: time="2025-10-21T13:33:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/78664ae1c86cffebb2c57c651cf0f56ba78bafa09429175e57f5be68cbf38d38/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 13:33:26 minikube dockerd[813]: time="2025-10-21T13:33:26.206971133Z" level=info msg="ignoring event" container=92f65186cdd268d2163f4a903c8ff6520a380e5f9cef00a1453edba5e2b35521 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:33:27 minikube dockerd[813]: time="2025-10-21T13:33:27.232077217Z" level=info msg="ignoring event" container=98e2789cde8226bc42cd9b1cd7b8e653ab25e99128177cf643c35f973a1e7946 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:33:41 minikube dockerd[813]: time="2025-10-21T13:33:41.243999292Z" level=info msg="ignoring event" container=1ae7de50cc89cef1c6c92126fc108e0bf3018d3e804febdc68dccb2c1c1b9f69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:34:11 minikube dockerd[813]: time="2025-10-21T13:34:11.361466001Z" level=info msg="ignoring event" container=ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:35:03 minikube dockerd[813]: time="2025-10-21T13:35:03.303769595Z" level=info msg="ignoring event" container=5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:36:26 minikube dockerd[813]: time="2025-10-21T13:36:26.333975133Z" level=info msg="ignoring event" container=a0c3dbb876f9b7c3b66654a7c8bdd0866a73daf8068cd0e662e3606104eb6267 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:36:35 minikube dockerd[813]: time="2025-10-21T13:36:35.371303262Z" level=info msg="ignoring event" container=78664ae1c86cffebb2c57c651cf0f56ba78bafa09429175e57f5be68cbf38d38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:37:04 minikube dockerd[813]: time="2025-10-21T13:37:04.340026845Z" level=info msg="ignoring event" container=07868bb517c89a1d8974b77c9976a1dc232d03fc817b6b52cd007c91f12f7688 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:37:04 minikube cri-dockerd[1144]: time="2025-10-21T13:37:04Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"postgres-pod_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Oct 21 13:37:04 minikube dockerd[813]: time="2025-10-21T13:37:04.537974429Z" level=info msg="ignoring event" container=60411b3f6c01ffa55253207df239f0004340cd6c426e0e8cf7eca4782a811ece module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 21 13:37:14 minikube cri-dockerd[1144]: time="2025-10-21T13:37:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9e5453a6fffca0c71163a724eb0f3584f5b7dfc3727ea59146e6e6320e65ee32/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 21 13:37:59 minikube cri-dockerd[1144]: time="2025-10-21T13:37:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/967513598db6a2b2b9390e09b826e110f5986914e552fa68b87afe74e1064b7b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE                                                                                                     CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
7b53b9789a4cf       2b2f04c7875c5                                                                                             8 minutes ago       Running             worker-app                0                   967513598db6a       worker-app-pod
97537395062c5       a6c8d67957cf2                                                                                             9 minutes ago       Running             postgres                  0                   9e5453a6fffca       postgres-pod
7d67f4dc56d06       redis@sha256:f0957bcaa75fd58a9a1847c1f07caf370579196259d69ac07f2e27b5b389b021                             24 hours ago        Running             redis                     0                   b3d0913e32896       redis-pod
dd42ce24ab3ba       kodekloud/examplevotingapp_vote@sha256:3a856afb02a37db0cc0e38ee047de510eba870ee76135df6b43c91eaeeff2836   24 hours ago        Running             voting-app                0                   e2600e494a624       voting-app-pod
f3fa245841465       ba04bb24b9575                                                                                             5 days ago          Running             storage-provisioner       11                  a35e89bde6167       storage-provisioner
00e61774f9994       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   5de0d626d5304       myapp-deployment-5f74d7f96-2fb7f
dbc2ef75d354c       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                5 days ago          Running             hello-web-container       4                   07d2b9bae46c8       hello-web-rs-nz4lk
24101abc5742c       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   2827ad239afac       myapp-deployment-5f74d7f96-ws7d5
3b90649584f92       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   2324b6b382e4a       myapp-deployment-5f74d7f96-nqbtk
3fb1d883bbf0f       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   09b3f7faa040e       myapp-deployment-5f74d7f96-pw7jr
8d29ab92b3e26       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   623a63aaafb4d       myapp-deployment-5f74d7f96-x7c57
ab2e93ee59e80       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                5 days ago          Running             hello-web-container       4                   2a6414f58ff78       hello-web-rs-sv8vh
49cad35aa976e       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                5 days ago          Running             hello-web-container       4                   dacc9447e493b       hello-web-rs-bmc5v
eb277d8881a92       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             5 days ago          Running             nginx                     1                   dbeb2d6a69951       myapp-deployment-5f74d7f96-vsvv5
1560a5aa4222b       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                5 days ago          Running             hello-web-container       4                   94019e5a66f38       hello-web-pod
8fb781f3b35c4       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                5 days ago          Running             hello-web-container       4                   8afd1c2ac8a39       hello-web-rs-br2fp
0f05d1559a593       138784d87c9c5                                                                                             5 days ago          Running             coredns                   5                   bc2918b4f0edf       coredns-66bc5c9577-74j4j
91a87e17c550f       ba04bb24b9575                                                                                             5 days ago          Exited              storage-provisioner       10                  a35e89bde6167       storage-provisioner
13ed0c21f56aa       6fc32d66c1411                                                                                             5 days ago          Running             kube-proxy                5                   61f242fae6023       kube-proxy-m7bh2
7cee4ba487f9c       a25f5ef9c34c3                                                                                             5 days ago          Running             kube-scheduler            5                   f36fd5c717d25       kube-scheduler-minikube
c91546125685b       d291939e99406                                                                                             5 days ago          Running             kube-apiserver            5                   3a94a6d627930       kube-apiserver-minikube
fc955108e2a10       996be7e86d9b3                                                                                             5 days ago          Running             kube-controller-manager   5                   e491bf2548db6       kube-controller-manager-minikube
81cb8114023f2       a1894772a478e                                                                                             5 days ago          Running             etcd                      5                   7f986dd0da248       etcd-minikube
9272570f35c29       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   854a7e2dbce94       myapp-deployment-5f74d7f96-ws7d5
73a5b4c1ab7f9       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   83443f4949815       myapp-deployment-5f74d7f96-nqbtk
51f34dd6771f4       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   18f5c1acee1bf       myapp-deployment-5f74d7f96-vsvv5
49f1c45b3e51d       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   787ee14f41a07       myapp-deployment-5f74d7f96-x7c57
0ecffd3f39c08       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   7c3ab27d8de02       myapp-deployment-5f74d7f96-2fb7f
3f06a9cddccff       nginx@sha256:3b7732505933ca591ce4a6d860cb713ad96a3176b82f7979a8dfa9973486a0d6                             7 days ago          Exited              nginx                     0                   668956c371e91       myapp-deployment-5f74d7f96-pw7jr
fb51bf0e82d4d       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                8 days ago          Exited              hello-web-container       3                   d5f5fb45ddfb9       hello-web-rs-sv8vh
5c58991d7962e       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                8 days ago          Exited              hello-web-container       3                   8e526d0138dcf       hello-web-rs-bmc5v
f77da632d0d07       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                8 days ago          Exited              hello-web-container       3                   4f5d828411be4       hello-web-pod
6dee6fbaa60db       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                8 days ago          Exited              hello-web-container       3                   0e56398fe8d68       hello-web-rs-br2fp
605daad86aece       gmidenta/hello-web@sha256:bdf443bb055165616a72dc63d2f7195ba11a71e9b8b3af2d41a1327916f85756                8 days ago          Exited              hello-web-container       3                   92fd90a3ab529       hello-web-rs-nz4lk
5c2196c446792       138784d87c9c5                                                                                             8 days ago          Exited              coredns                   4                   f464dcb518976       coredns-66bc5c9577-74j4j
cee6e963ae679       6fc32d66c1411                                                                                             8 days ago          Exited              kube-proxy                4                   fdf3de7ac938f       kube-proxy-m7bh2
11e776feed990       996be7e86d9b3                                                                                             8 days ago          Exited              kube-controller-manager   4                   d17f80880e85f       kube-controller-manager-minikube
d85f40924c898       a25f5ef9c34c3                                                                                             8 days ago          Exited              kube-scheduler            4                   1d1697f80ddea       kube-scheduler-minikube
06237f4495190       d291939e99406                                                                                             8 days ago          Exited              kube-apiserver            4                   216c04ef13b39       kube-apiserver-minikube
410221a6bdf91       a1894772a478e                                                                                             8 days ago          Exited              etcd                      4                   b243abddd5ce3       etcd-minikube


==> coredns [0f05d1559a59] <==
maxprocs: Leaving GOMAXPROCS=11: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/arm64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:48926 - 57803 "HINFO IN 2873374248444607362.7185873868180751020. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.044634167s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] 10.244.0.80:42691 - 2193 "AAAA IN redis.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000883583s
[INFO] 10.244.0.80:42691 - 11684 "A IN redis.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.001063417s
[INFO] 10.244.0.80:42946 - 3340 "AAAA IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000045875s
[INFO] 10.244.0.80:42946 - 51211 "A IN redis.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000035166s
[INFO] 10.244.0.80:44376 - 58076 "A IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.00005575s
[INFO] 10.244.0.80:44376 - 8667 "AAAA IN redis.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000100833s
[INFO] 10.244.0.80:50737 - 29100 "A IN redis. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.012281709s
[INFO] 10.244.0.80:50737 - 26019 "AAAA IN redis. udp 23 false 512" NXDOMAIN qr,rd,ra 23 0.012370042s
[INFO] 10.244.0.88:50079 - 25168 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000702792s
[INFO] 10.244.0.88:50079 - 217 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000762542s
[INFO] 10.244.0.88:60929 - 62570 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000087291s
[INFO] 10.244.0.88:60929 - 55658 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000114875s
[INFO] 10.244.0.88:53427 - 2941 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000131125s
[INFO] 10.244.0.88:53427 - 6070 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000209667s
[INFO] 10.244.0.88:47317 - 55465 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000249792s
[INFO] 10.244.0.88:47317 - 28164 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000314875s
[INFO] 10.244.0.89:53785 - 37682 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000478542s
[INFO] 10.244.0.89:53785 - 20403 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.0004995s
[INFO] 10.244.0.89:43990 - 29131 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.0000895s
[INFO] 10.244.0.89:43990 - 64532 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000114334s
[INFO] 10.244.0.89:44076 - 43802 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000101666s
[INFO] 10.244.0.89:44076 - 51671 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000139042s
[INFO] 10.244.0.89:35984 - 53926 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.00047225s
[INFO] 10.244.0.89:35984 - 12158 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000592667s
[INFO] 10.244.0.89:34150 - 62236 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000259417s
[INFO] 10.244.0.89:34150 - 47795 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000335292s
[INFO] 10.244.0.89:58255 - 50719 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.000455125s
[INFO] 10.244.0.89:58255 - 60423 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.000552667s
[INFO] 10.244.0.91:58669 - 15078 "AAAA IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 139 0.001000292s
[INFO] 10.244.0.91:58669 - 27977 "A IN db.default.svc.cluster.local. udp 46 false 512" NOERROR qr,aa,rd 90 0.001062459s
[INFO] 10.244.0.91:33263 - 33437 "A IN redis.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000095958s
[INFO] 10.244.0.91:33263 - 55616 "AAAA IN redis.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.0001255s


==> coredns [5c2196c44679] <==
maxprocs: Leaving GOMAXPROCS=11: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/arm64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:37882 - 17685 "HINFO IN 2667676295120776087.4025037627486500434. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.105628292s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_09_30T13_56_42_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 30 Sep 2025 11:56:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 21 Oct 2025 13:46:25 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 21 Oct 2025 13:46:00 +0000   Tue, 30 Sep 2025 11:56:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 21 Oct 2025 13:46:00 +0000   Tue, 30 Sep 2025 11:56:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 21 Oct 2025 13:46:00 +0000   Tue, 30 Sep 2025 11:56:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 21 Oct 2025 13:46:00 +0000   Tue, 30 Sep 2025 11:56:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                11
  ephemeral-storage:  474095688Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025284Ki
  pods:               110
Allocatable:
  cpu:                11
  ephemeral-storage:  474095688Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025284Ki
  pods:               110
System Info:
  Machine ID:                 518a0922e6bb4106b090f8d6b0c8c5ee
  System UUID:                518a0922e6bb4106b090f8d6b0c8c5ee
  Boot ID:                    4f17542f-5e2c-4935-9028-309716cc83ef
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (22 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     hello-web-pod                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  default                     hello-web-rs-bmc5v                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  default                     hello-web-rs-br2fp                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  default                     hello-web-rs-nz4lk                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  default                     hello-web-rs-sv8vh                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18d
  default                     myapp-deployment-5f74d7f96-2fb7f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     myapp-deployment-5f74d7f96-nqbtk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     myapp-deployment-5f74d7f96-pw7jr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     myapp-deployment-5f74d7f96-vsvv5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     myapp-deployment-5f74d7f96-ws7d5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     myapp-deployment-5f74d7f96-x7c57    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d22h
  default                     postgres-pod                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m17s
  default                     redis-pod                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  default                     voting-app-pod                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  default                     worker-app-pod                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m32s
  kube-system                 coredns-66bc5c9577-74j4j            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     21d
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         21d
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         21d
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         21d
  kube-system                 kube-proxy-m7bh2                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21d
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         21d
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         21d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Oct21 01:40] netlink: 'init': attribute type 4 has an invalid length.
[  +0.205386] fakeowner: loading out-of-tree module taints kernel.


==> etcd [410221a6bdf9] <==
{"level":"info","ts":"2025-10-14T14:14:48.137504Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":72212}
{"level":"info","ts":"2025-10-14T14:14:48.140566Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":72212,"took":"2.742291ms","hash":611275052,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1441792,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:14:48.140624Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":611275052,"revision":72212,"compact-revision":71973}
{"level":"info","ts":"2025-10-14T14:19:48.142541Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":72453}
{"level":"info","ts":"2025-10-14T14:19:48.145582Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":72453,"took":"2.499916ms","hash":2222164023,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1388544,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:19:48.145671Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2222164023,"revision":72453,"compact-revision":72212}
{"level":"info","ts":"2025-10-14T14:24:48.148210Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":72692}
{"level":"info","ts":"2025-10-14T14:24:48.150557Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":72692,"took":"2.080667ms","hash":3709317253,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:24:48.150616Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3709317253,"revision":72692,"compact-revision":72453}
{"level":"info","ts":"2025-10-14T14:29:48.154182Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":72932}
{"level":"info","ts":"2025-10-14T14:29:48.157493Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":72932,"took":"2.771125ms","hash":767055648,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:29:48.157585Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":767055648,"revision":72932,"compact-revision":72692}
{"level":"info","ts":"2025-10-14T14:34:48.186091Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":73172}
{"level":"info","ts":"2025-10-14T14:34:48.189285Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":73172,"took":"2.665459ms","hash":3454139784,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1441792,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:34:48.189361Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3454139784,"revision":73172,"compact-revision":72932}
{"level":"info","ts":"2025-10-14T14:39:48.192853Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":73411}
{"level":"info","ts":"2025-10-14T14:39:48.201606Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":73411,"took":"8.130041ms","hash":2510351310,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1445888,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:39:48.201699Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2510351310,"revision":73411,"compact-revision":73172}
{"level":"info","ts":"2025-10-14T14:44:48.203561Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":73651}
{"level":"info","ts":"2025-10-14T14:44:48.207307Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":73651,"took":"3.048334ms","hash":3058172430,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1437696,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T14:44:48.207423Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3058172430,"revision":73651,"compact-revision":73411}
{"level":"info","ts":"2025-10-14T14:49:48.210863Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":73891}
{"level":"info","ts":"2025-10-14T14:49:48.213173Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":73891,"took":"2.088042ms","hash":1107870418,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-14T14:49:48.213214Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1107870418,"revision":73891,"compact-revision":73651}
{"level":"info","ts":"2025-10-14T14:54:48.217307Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":74130}
{"level":"info","ts":"2025-10-14T14:54:48.220609Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":74130,"took":"2.749958ms","hash":971748340,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1462272,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-14T14:54:48.220717Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":971748340,"revision":74130,"compact-revision":73891}
{"level":"info","ts":"2025-10-14T14:59:48.225042Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":74371}
{"level":"info","ts":"2025-10-14T14:59:48.228200Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":74371,"took":"2.930167ms","hash":1089661306,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1454080,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-14T14:59:48.228262Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1089661306,"revision":74371,"compact-revision":74130}
{"level":"info","ts":"2025-10-14T15:04:48.246176Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":74610}
{"level":"info","ts":"2025-10-14T15:04:48.248104Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":74610,"took":"1.584334ms","hash":4242562265,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1462272,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-14T15:04:48.248157Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4242562265,"revision":74610,"compact-revision":74371}
{"level":"info","ts":"2025-10-14T15:09:48.254563Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":74851}
{"level":"info","ts":"2025-10-14T15:09:48.257498Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":74851,"took":"2.499333ms","hash":343728067,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T15:09:48.257575Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":343728067,"revision":74851,"compact-revision":74610}
{"level":"info","ts":"2025-10-14T15:10:10.238960Z","caller":"traceutil/trace.go:172","msg":"trace[1779107963] transaction","detail":"{read_only:false; response_revision:75108; number_of_response:1; }","duration":"107.750292ms","start":"2025-10-14T15:10:10.131195Z","end":"2025-10-14T15:10:10.238946Z","steps":["trace[1779107963] 'process raft request'  (duration: 107.652667ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T15:30:40.422482Z","caller":"traceutil/trace.go:172","msg":"trace[1669547479] transaction","detail":"{read_only:false; response_revision:75124; number_of_response:1; }","duration":"110.874541ms","start":"2025-10-14T15:30:40.311592Z","end":"2025-10-14T15:30:40.422467Z","steps":["trace[1669547479] 'process raft request'  (duration: 110.786833ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T15:30:45.784060Z","caller":"traceutil/trace.go:172","msg":"trace[1457612424] transaction","detail":"{read_only:false; response_revision:75128; number_of_response:1; }","duration":"112.041542ms","start":"2025-10-14T15:30:45.672004Z","end":"2025-10-14T15:30:45.784046Z","steps":["trace[1457612424] 'process raft request'  (duration: 111.758792ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T16:18:14.664306Z","caller":"traceutil/trace.go:172","msg":"trace[605170564] transaction","detail":"{read_only:false; response_revision:75140; number_of_response:1; }","duration":"111.932125ms","start":"2025-10-14T16:18:14.552360Z","end":"2025-10-14T16:18:14.664292Z","steps":["trace[605170564] 'process raft request'  (duration: 111.809625ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T16:22:14.938448Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":75091}
{"level":"info","ts":"2025-10-14T16:22:14.942603Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":75091,"took":"3.513167ms","hash":1628380981,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T16:22:14.942704Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1628380981,"revision":75091,"compact-revision":74851}
{"level":"info","ts":"2025-10-14T16:27:39.055403Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":75329}
{"level":"info","ts":"2025-10-14T16:27:39.060116Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":75329,"took":"3.971ms","hash":2390266857,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T16:27:39.060247Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2390266857,"revision":75329,"compact-revision":75091}
{"level":"info","ts":"2025-10-14T16:31:51.525216Z","caller":"traceutil/trace.go:172","msg":"trace[18881113] linearizableReadLoop","detail":"{readStateIndex:94483; appliedIndex:94483; }","duration":"114.560417ms","start":"2025-10-14T16:31:51.410633Z","end":"2025-10-14T16:31:51.525193Z","steps":["trace[18881113] 'read index received'  (duration: 114.556917ms)","trace[18881113] 'applied index is now lower than readState.Index'  (duration: 2.958¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-14T16:31:51.525479Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"114.77725ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-10-14T16:31:51.525519Z","caller":"traceutil/trace.go:172","msg":"trace[1862971202] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:75708; }","duration":"114.892333ms","start":"2025-10-14T16:31:51.410619Z","end":"2025-10-14T16:31:51.525511Z","steps":["trace[1862971202] 'agreement among raft nodes before linearized reading'  (duration: 114.753958ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T17:33:53.193797Z","caller":"traceutil/trace.go:172","msg":"trace[361630480] transaction","detail":"{read_only:false; response_revision:75767; number_of_response:1; }","duration":"105.572917ms","start":"2025-10-14T17:33:53.088198Z","end":"2025-10-14T17:33:53.193771Z","steps":["trace[361630480] 'process raft request'  (duration: 105.478625ms)"],"step_count":1}
{"level":"info","ts":"2025-10-14T18:32:16.691162Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":75569}
{"level":"info","ts":"2025-10-14T18:32:16.693027Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":75569,"took":"1.595125ms","hash":3788612908,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1425408,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T18:32:16.693062Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3788612908,"revision":75569,"compact-revision":75329}
{"level":"info","ts":"2025-10-14T21:47:08.861415Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":75809}
{"level":"info","ts":"2025-10-14T21:47:08.864548Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":75809,"took":"2.630083ms","hash":3233512137,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1421312,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-10-14T21:47:08.864658Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3233512137,"revision":75809,"compact-revision":75569}
{"level":"info","ts":"2025-10-14T21:50:13.721138Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-10-14T21:50:13.721324Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2025-10-14T21:50:13.721508Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-10-14T21:50:13.721640Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}


==> etcd [81cb8114023f] <==
{"level":"info","ts":"2025-10-21T09:32:33.046812Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":108193,"took":"3.14075ms","hash":2348186308,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1609728,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-10-21T09:32:33.046862Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2348186308,"revision":108193,"compact-revision":107947}
{"level":"info","ts":"2025-10-21T09:37:33.048833Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":108459}
{"level":"info","ts":"2025-10-21T09:37:33.053183Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":108459,"took":"3.601375ms","hash":2693628891,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1609728,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-10-21T09:37:33.053264Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2693628891,"revision":108459,"compact-revision":108193}
{"level":"info","ts":"2025-10-21T10:21:33.133564Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":108724}
{"level":"info","ts":"2025-10-21T10:21:33.137509Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":108724,"took":"3.364917ms","hash":343149935,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T10:21:33.137551Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":343149935,"revision":108724,"compact-revision":108459}
{"level":"warn","ts":"2025-10-21T11:33:33.946509Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"117.614209ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-10-21T11:33:33.946726Z","caller":"traceutil/trace.go:172","msg":"trace[1608808366] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:109014; }","duration":"117.895667ms","start":"2025-10-21T11:33:33.828821Z","end":"2025-10-21T11:33:33.946716Z","steps":["trace[1608808366] 'range keys from in-memory index tree'  (duration: 117.541459ms)"],"step_count":1}
{"level":"warn","ts":"2025-10-21T11:33:33.946509Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"117.451042ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" limit:10000 revision:109012 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-10-21T11:33:33.946790Z","caller":"traceutil/trace.go:172","msg":"trace[373033867] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:109014; }","duration":"117.799292ms","start":"2025-10-21T11:33:33.828984Z","end":"2025-10-21T11:33:33.946783Z","steps":["trace[373033867] 'range keys from in-memory index tree'  (duration: 117.416917ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T12:29:01.904954Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":108964}
{"level":"info","ts":"2025-10-21T12:29:01.909088Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":108964,"took":"3.456708ms","hash":2692456365,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1527808,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T12:29:01.909148Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2692456365,"revision":108964,"compact-revision":108724}
{"level":"info","ts":"2025-10-21T12:32:50.419010Z","caller":"traceutil/trace.go:172","msg":"trace[1488541118] transaction","detail":"{read_only:false; response_revision:109388; number_of_response:1; }","duration":"142.424375ms","start":"2025-10-21T12:32:50.276570Z","end":"2025-10-21T12:32:50.418994Z","steps":["trace[1488541118] 'process raft request'  (duration: 142.302833ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:01:29.749935Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":109203}
{"level":"info","ts":"2025-10-21T13:01:29.752990Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":109203,"took":"2.590584ms","hash":211251569,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1523712,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:01:29.753028Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":211251569,"revision":109203,"compact-revision":108964}
{"level":"info","ts":"2025-10-21T13:06:29.755156Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":109445}
{"level":"info","ts":"2025-10-21T13:06:29.757320Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":109445,"took":"1.904ms","hash":1314724709,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1507328,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:06:29.757363Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1314724709,"revision":109445,"compact-revision":109203}
{"level":"info","ts":"2025-10-21T13:11:29.762763Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":109685}
{"level":"info","ts":"2025-10-21T13:11:29.766238Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":109685,"took":"2.949792ms","hash":1655790400,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1515520,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:11:29.766284Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1655790400,"revision":109685,"compact-revision":109445}
{"level":"info","ts":"2025-10-21T13:16:29.764471Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":109924}
{"level":"info","ts":"2025-10-21T13:16:29.765517Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":109924,"took":"916.416¬µs","hash":1343261611,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1466368,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:16:29.765534Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1343261611,"revision":109924,"compact-revision":109685}
{"level":"warn","ts":"2025-10-21T13:16:49.207942Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"178.917916ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-10-21T13:16:49.208138Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"179.857959ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-10-21T13:16:49.208173Z","caller":"traceutil/trace.go:172","msg":"trace[1819823580] range","detail":"{range_begin:/registry/horizontalpodautoscalers; range_end:; response_count:0; response_revision:110181; }","duration":"179.198583ms","start":"2025-10-21T13:16:49.028945Z","end":"2025-10-21T13:16:49.208144Z","steps":["trace[1819823580] 'range keys from in-memory index tree'  (duration: 178.847833ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:16:49.208177Z","caller":"traceutil/trace.go:172","msg":"trace[1329784454] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:110181; }","duration":"179.915791ms","start":"2025-10-21T13:16:49.028255Z","end":"2025-10-21T13:16:49.208171Z","steps":["trace[1329784454] 'range keys from in-memory index tree'  (duration: 179.850667ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:16:51.866495Z","caller":"traceutil/trace.go:172","msg":"trace[1264051391] linearizableReadLoop","detail":"{readStateIndex:137643; appliedIndex:137643; }","duration":"108.173458ms","start":"2025-10-21T13:16:51.758270Z","end":"2025-10-21T13:16:51.866443Z","steps":["trace[1264051391] 'read index received'  (duration: 108.168292ms)","trace[1264051391] 'applied index is now lower than readState.Index'  (duration: 4.333¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-10-21T13:16:51.867888Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"109.603542ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-10-21T13:16:51.868140Z","caller":"traceutil/trace.go:172","msg":"trace[68556554] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions; range_end:; response_count:0; response_revision:110181; }","duration":"110.14375ms","start":"2025-10-21T13:16:51.757971Z","end":"2025-10-21T13:16:51.868114Z","steps":["trace[68556554] 'agreement among raft nodes before linearized reading'  (duration: 109.832625ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:17:11.325812Z","caller":"traceutil/trace.go:172","msg":"trace[1243281572] transaction","detail":"{read_only:false; response_revision:110197; number_of_response:1; }","duration":"122.592625ms","start":"2025-10-21T13:17:11.203209Z","end":"2025-10-21T13:17:11.325802Z","steps":["trace[1243281572] 'process raft request'  (duration: 119.128417ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:21:29.774578Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":110165}
{"level":"info","ts":"2025-10-21T13:21:29.778209Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":110165,"took":"2.933458ms","hash":4050234895,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1490944,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:21:29.778266Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4050234895,"revision":110165,"compact-revision":109924}
{"level":"info","ts":"2025-10-21T13:26:29.776635Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":110403}
{"level":"info","ts":"2025-10-21T13:26:29.779582Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":110403,"took":"2.564166ms","hash":3257191324,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1478656,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:26:29.779622Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3257191324,"revision":110403,"compact-revision":110165}
{"level":"info","ts":"2025-10-21T13:29:53.773320Z","caller":"traceutil/trace.go:172","msg":"trace[1397798371] transaction","detail":"{read_only:false; response_revision:110804; number_of_response:1; }","duration":"117.051167ms","start":"2025-10-21T13:29:53.656251Z","end":"2025-10-21T13:29:53.773302Z","steps":["trace[1397798371] 'process raft request'  (duration: 116.913792ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:29:55.295051Z","caller":"traceutil/trace.go:172","msg":"trace[237508551] transaction","detail":"{read_only:false; response_revision:110806; number_of_response:1; }","duration":"133.487625ms","start":"2025-10-21T13:29:55.161555Z","end":"2025-10-21T13:29:55.295043Z","steps":["trace[237508551] 'process raft request'  (duration: 133.316667ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:31:15.030717Z","caller":"traceutil/trace.go:172","msg":"trace[31277911] transaction","detail":"{read_only:false; response_revision:110874; number_of_response:1; }","duration":"126.661625ms","start":"2025-10-21T13:31:14.904035Z","end":"2025-10-21T13:31:15.030696Z","steps":["trace[31277911] 'process raft request'  (duration: 126.532958ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:31:16.726219Z","caller":"traceutil/trace.go:172","msg":"trace[2008812748] transaction","detail":"{read_only:false; response_revision:110876; number_of_response:1; }","duration":"117.513417ms","start":"2025-10-21T13:31:16.608695Z","end":"2025-10-21T13:31:16.726208Z","steps":["trace[2008812748] 'process raft request'  (duration: 117.301209ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:31:21.204174Z","caller":"traceutil/trace.go:172","msg":"trace[1417700151] transaction","detail":"{read_only:false; response_revision:110880; number_of_response:1; }","duration":"112.253042ms","start":"2025-10-21T13:31:21.091892Z","end":"2025-10-21T13:31:21.204145Z","steps":["trace[1417700151] 'process raft request'  (duration: 112.161625ms)"],"step_count":1}
{"level":"info","ts":"2025-10-21T13:31:29.785246Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":110643}
{"level":"info","ts":"2025-10-21T13:31:29.789802Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":110643,"took":"3.778416ms","hash":4250586,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1503232,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-10-21T13:31:29.789855Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4250586,"revision":110643,"compact-revision":110403}
{"level":"info","ts":"2025-10-21T13:36:29.792735Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":110899}
{"level":"info","ts":"2025-10-21T13:36:29.799035Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":110899,"took":"5.444125ms","hash":3481154489,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-10-21T13:36:29.799097Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3481154489,"revision":110899,"compact-revision":110643}
{"level":"info","ts":"2025-10-21T13:41:29.799958Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":111209}
{"level":"info","ts":"2025-10-21T13:41:29.804256Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":111209,"took":"3.48325ms","hash":2394209531,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1744896,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-10-21T13:41:29.804305Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2394209531,"revision":111209,"compact-revision":110899}
{"level":"info","ts":"2025-10-21T13:42:13.294031Z","caller":"wal/wal.go:825","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000002-0000000000022033.wal"}
{"level":"info","ts":"2025-10-21T13:46:29.808882Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":111490}
{"level":"info","ts":"2025-10-21T13:46:29.814233Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":111490,"took":"4.2295ms","hash":1831904122,"current-db-size-bytes":3084288,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1622016,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-10-21T13:46:29.814296Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1831904122,"revision":111490,"compact-revision":111209}


==> kernel <==
 13:46:31 up 12:06,  0 users,  load average: 2.98, 2.79, 2.93
Linux minikube 6.10.14-linuxkit #1 SMP Sat May 17 08:28:57 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [06237f449519] <==
W1014 21:50:13.723270       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.723316       1 logging.go:55] [core] [Channel #251 SubChannel #253]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.723400       1 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.723577       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.723706       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724036       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724054       1 logging.go:55] [core] [Channel #227 SubChannel #229]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724076       1 logging.go:55] [core] [Channel #235 SubChannel #2525]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724093       1 logging.go:55] [core] [Channel #87 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724117       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724132       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724179       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724196       1 logging.go:55] [core] [Channel #183 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724217       1 logging.go:55] [core] [Channel #83 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724945       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724969       1 logging.go:55] [core] [Channel #2 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726214       1 logging.go:55] [core] [Channel #127 SubChannel #129]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.724988       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725009       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725024       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725030       1 logging.go:55] [core] [Channel #31 SubChannel #2530]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726303       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726305       1 logging.go:55] [core] [Channel #67 SubChannel #69]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725044       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725044       1 logging.go:55] [core] [Channel #1 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725067       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725103       1 logging.go:55] [core] [Channel #59 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725082       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725115       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725124       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725137       1 logging.go:55] [core] [Channel #39 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725176       1 logging.go:55] [core] [Channel #143 SubChannel #145]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725337       1 logging.go:55] [core] [Channel #223 SubChannel #225]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725355       1 logging.go:55] [core] [Channel #151 SubChannel #153]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725420       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725455       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725475       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725532       1 logging.go:55] [core] [Channel #207 SubChannel #209]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725551       1 logging.go:55] [core] [Channel #111 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725552       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725631       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725796       1 logging.go:55] [core] [Channel #27 SubChannel #933]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725814       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725838       1 logging.go:55] [core] [Channel #199 SubChannel #201]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726478       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725865       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725883       1 logging.go:55] [core] [Channel #13 SubChannel #15]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725950       1 logging.go:55] [core] [Channel #99 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725970       1 logging.go:55] [core] [Channel #47 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725987       1 logging.go:55] [core] [Channel #119 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725993       1 logging.go:55] [core] [Channel #21 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726097       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726120       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726133       1 logging.go:55] [core] [Channel #95 SubChannel #97]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726139       1 logging.go:55] [core] [Channel #163 SubChannel #165]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726142       1 logging.go:55] [core] [Channel #187 SubChannel #189]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726236       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725232       1 logging.go:55] [core] [Channel #167 SubChannel #169]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.725377       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1014 21:50:13.726730       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [c91546125685] <==
I1021 13:11:29.155365       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1021 13:12:25.276026       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:12:38.704203       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:13:48.742019       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:13:57.220955       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:14:57.222142       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:14:58.879345       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:16:17.003640       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:16:21.498479       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:17:42.581906       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:17:46.611658       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:18:49.958260       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:19:07.410026       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:20:04.645639       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:20:14.283577       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:21:14.565479       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:21:29.158409       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1021 13:21:30.248609       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:22:39.472823       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:22:48.856740       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:23:55.202797       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:24:10.343935       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:25:12.545883       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:25:31.503899       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:26:18.462492       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:26:42.093949       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:27:23.547424       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:28:02.654070       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:28:39.497196       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:29:07.060997       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:29:46.815970       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:30:27.384092       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:31:15.516254       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:31:29.152776       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1021 13:31:50.981865       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:32:29.885247       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:32:58.369241       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:33:58.561945       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:34:07.905467       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:35:08.085324       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:35:32.284943       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:36:30.110205       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:36:41.355724       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:37:47.599817       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:38:06.774289       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:38:57.683941       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:39:24.487773       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:39:36.397792       1 alloc.go:328] "allocated clusterIPs" service="default/result-service" clusterIPs={"IPv4":"10.102.99.227"}
I1021 13:40:13.528026       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:40:39.688414       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:41:29.153511       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1021 13:41:33.505434       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:42:02.962497       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:42:40.900291       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:43:09.772463       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:43:49.472683       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:44:29.076393       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:44:54.235748       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:45:42.910979       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1021 13:46:12.278390       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [11e776feed99] <==
I1013 13:23:32.782915       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1013 13:23:32.782972       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1013 13:23:32.783916       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1013 13:23:32.787908       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1013 13:23:32.787927       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1013 13:23:32.789812       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1013 13:23:32.789834       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1013 13:23:32.791718       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1013 13:23:32.793459       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1013 13:23:32.796493       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1013 13:23:32.796509       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1013 13:23:32.796958       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1013 13:23:32.797017       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1013 13:23:32.797159       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1013 13:23:32.797165       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1013 13:23:32.797934       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1013 13:23:32.797958       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1013 13:23:32.800041       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1013 13:23:32.800057       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1013 13:23:32.800063       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1013 13:23:32.802507       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1013 13:23:32.802525       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1013 13:23:32.802744       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1013 13:23:32.802767       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1013 13:23:32.802787       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1013 13:23:32.802873       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1013 13:23:32.803519       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1013 13:23:32.805671       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1013 13:23:32.805687       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1013 13:23:32.805696       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1013 13:23:32.805701       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
E1013 13:32:02.948467       1 replica_set.go:587] "Unhandled Error" err="sync \"default/myapp-deployment-54f68f64b4\" failed with Operation cannot be fulfilled on replicasets.apps \"myapp-deployment-54f68f64b4\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
I1013 14:40:41.119517       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1013 14:40:41.119922       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1013 16:38:01.807828       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1013 16:38:01.807980       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1013 18:13:52.215772       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1013 18:13:52.215992       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1013 19:34:14.199586       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1013 19:34:14.199600       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1013 21:16:28.520544       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1013 21:16:28.520586       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1014 00:50:18.757559       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 00:50:18.757589       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 03:17:43.044519       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 03:17:43.044943       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1014 05:53:32.446222       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 05:53:32.446529       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 08:23:56.663079       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 08:23:56.663097       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 14:32:28.430311       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 14:32:28.430667       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 16:18:22.832198       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 16:18:22.832278       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1014 17:33:56.729296       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 17:33:56.729891       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1014 19:32:04.369740       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 19:32:04.369843       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1014 21:34:05.384925       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1014 21:34:05.384940       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-controller-manager [fc955108e2a1] <==
I1018 19:25:00.680628       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1018 19:25:00.680718       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1018 22:13:08.359108       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1018 22:13:08.359241       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1019 01:33:46.333390       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 01:33:46.333456       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 03:36:06.917123       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 03:36:06.917354       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 05:53:36.623539       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 05:53:36.623638       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1019 08:23:10.535048       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 08:23:10.535118       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1019 10:25:13.150351       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 10:25:13.150475       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1019 11:38:09.553486       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 11:38:09.553503       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 14:29:09.962678       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 14:29:09.962739       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1019 16:31:11.951326       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 16:31:11.951451       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1019 18:48:36.420886       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 18:48:36.421108       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1019 21:07:38.345214       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 21:07:38.345656       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1019 22:51:30.761222       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1019 22:51:30.761486       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 01:39:12.766967       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 01:39:12.767075       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1020 04:02:56.943885       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 04:02:56.943970       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1020 06:14:39.053998       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 06:14:39.054421       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1020 09:12:59.601709       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 09:12:59.601927       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 10:38:34.952634       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 10:38:34.952694       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 15:04:46.813883       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 15:04:46.813892       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1020 16:08:00.729253       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 16:08:00.729594       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 18:08:11.326127       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 18:08:11.326180       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 20:42:28.768234       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1020 20:42:28.768643       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1020 22:44:29.765689       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1020 22:44:29.765750       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1021 01:19:26.900563       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1021 01:19:26.900681       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1021 04:05:22.895353       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1021 04:05:22.895560       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1021 06:25:52.024218       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1021 06:25:52.024620       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1021 08:54:45.631750       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1021 08:54:45.631591       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1021 09:56:31.937711       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1021 09:56:31.938108       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
I1021 11:11:29.691723       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E1021 11:11:29.692173       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E1021 13:00:43.223829       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1021 13:00:43.224094       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-proxy [13ed0c21f56a] <==
I1016 10:33:47.659963       1 server_linux.go:53] "Using iptables proxy"
I1016 10:33:47.739162       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1016 10:33:47.840292       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1016 10:33:47.840318       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1016 10:33:47.840361       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1016 10:33:47.854844       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1016 10:33:47.854873       1 server_linux.go:132] "Using iptables Proxier"
I1016 10:33:47.857497       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1016 10:33:47.858026       1 server.go:527] "Version info" version="v1.34.0"
I1016 10:33:47.858043       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1016 10:33:47.860639       1 config.go:200] "Starting service config controller"
I1016 10:33:47.860651       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1016 10:33:47.860660       1 config.go:106] "Starting endpoint slice config controller"
I1016 10:33:47.860662       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1016 10:33:47.860670       1 config.go:403] "Starting serviceCIDR config controller"
I1016 10:33:47.860671       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1016 10:33:47.860934       1 config.go:309] "Starting node config controller"
I1016 10:33:47.860938       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1016 10:33:47.860940       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1016 10:33:47.961142       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1016 10:33:47.961162       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1016 10:33:47.961153       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-proxy [cee6e963ae67] <==
I1013 13:23:31.724998       1 server_linux.go:53] "Using iptables proxy"
I1013 13:23:31.816481       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1013 13:23:31.917550       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1013 13:23:31.917573       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1013 13:23:31.917619       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1013 13:23:31.930751       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1013 13:23:31.930779       1 server_linux.go:132] "Using iptables Proxier"
I1013 13:23:31.932797       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1013 13:23:31.933671       1 server.go:527] "Version info" version="v1.34.0"
I1013 13:23:31.933690       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 13:23:31.934401       1 config.go:200] "Starting service config controller"
I1013 13:23:31.934510       1 config.go:106] "Starting endpoint slice config controller"
I1013 13:23:31.934590       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1013 13:23:31.934412       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1013 13:23:31.935200       1 config.go:403] "Starting serviceCIDR config controller"
I1013 13:23:31.935216       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1013 13:23:31.935542       1 config.go:309] "Starting node config controller"
I1013 13:23:31.935554       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1013 13:23:32.034906       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1013 13:23:32.035536       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1013 13:23:32.035576       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1013 13:23:32.035611       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [7cee4ba487f9] <==
I1016 10:33:44.045232       1 serving.go:386] Generated self-signed cert in-memory
W1016 10:33:45.471123       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1016 10:33:45.471161       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1016 10:33:45.471168       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1016 10:33:45.471172       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1016 10:33:45.545635       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1016 10:33:45.545652       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1016 10:33:45.549000       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1016 10:33:45.549046       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1016 10:33:45.549127       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1016 10:33:45.549307       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1016 10:33:45.650041       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [d85f40924c89] <==
I1013 13:23:28.382199       1 serving.go:386] Generated self-signed cert in-memory
W1013 13:23:29.391789       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1013 13:23:29.391862       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1013 13:23:29.391874       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1013 13:23:29.391881       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1013 13:23:29.399342       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1013 13:23:29.399355       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 13:23:29.480077       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 13:23:29.480271       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1013 13:23:29.480396       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1013 13:23:29.480395       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 13:23:29.580569       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1014 21:50:13.820400       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I1014 21:50:13.820480       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I1014 21:50:13.820507       1 server.go:265] "[graceful-termination] secure server is exiting"
E1014 21:50:13.820604       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Oct 21 13:33:25 minikube kubelet[1371]: I1021 13:33:25.299703    1371 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wvxgf\" (UniqueName: \"kubernetes.io/projected/b7956bf8-d67d-4435-81ca-667f0c8afb00-kube-api-access-wvxgf\") pod \"worker-app-pod\" (UID: \"b7956bf8-d67d-4435-81ca-667f0c8afb00\") " pod="default/worker-app-pod"
Oct 21 13:33:26 minikube kubelet[1371]: I1021 13:33:26.668193    1371 scope.go:117] "RemoveContainer" containerID="92f65186cdd268d2163f4a903c8ff6520a380e5f9cef00a1453edba5e2b35521"
Oct 21 13:33:27 minikube kubelet[1371]: I1021 13:33:27.697065    1371 scope.go:117] "RemoveContainer" containerID="92f65186cdd268d2163f4a903c8ff6520a380e5f9cef00a1453edba5e2b35521"
Oct 21 13:33:27 minikube kubelet[1371]: I1021 13:33:27.697324    1371 scope.go:117] "RemoveContainer" containerID="98e2789cde8226bc42cd9b1cd7b8e653ab25e99128177cf643c35f973a1e7946"
Oct 21 13:33:27 minikube kubelet[1371]: E1021 13:33:27.697452    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 10s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:33:28 minikube kubelet[1371]: I1021 13:33:28.723317    1371 scope.go:117] "RemoveContainer" containerID="98e2789cde8226bc42cd9b1cd7b8e653ab25e99128177cf643c35f973a1e7946"
Oct 21 13:33:28 minikube kubelet[1371]: E1021 13:33:28.723455    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 10s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:33:40 minikube kubelet[1371]: I1021 13:33:40.672665    1371 scope.go:117] "RemoveContainer" containerID="98e2789cde8226bc42cd9b1cd7b8e653ab25e99128177cf643c35f973a1e7946"
Oct 21 13:33:40 minikube kubelet[1371]: I1021 13:33:40.931994    1371 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/worker-app-pod" podStartSLOduration=15.931982459 podStartE2EDuration="15.931982459s" podCreationTimestamp="2025-10-21 13:33:25 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-21 13:33:40.931851376 +0000 UTC m=+42734.393835430" watchObservedRunningTime="2025-10-21 13:33:40.931982459 +0000 UTC m=+42734.393966596"
Oct 21 13:33:41 minikube kubelet[1371]: I1021 13:33:41.961526    1371 scope.go:117] "RemoveContainer" containerID="98e2789cde8226bc42cd9b1cd7b8e653ab25e99128177cf643c35f973a1e7946"
Oct 21 13:33:41 minikube kubelet[1371]: I1021 13:33:41.961754    1371 scope.go:117] "RemoveContainer" containerID="1ae7de50cc89cef1c6c92126fc108e0bf3018d3e804febdc68dccb2c1c1b9f69"
Oct 21 13:33:41 minikube kubelet[1371]: E1021 13:33:41.961899    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:33:56 minikube kubelet[1371]: I1021 13:33:56.657145    1371 scope.go:117] "RemoveContainer" containerID="1ae7de50cc89cef1c6c92126fc108e0bf3018d3e804febdc68dccb2c1c1b9f69"
Oct 21 13:33:56 minikube kubelet[1371]: E1021 13:33:56.657537    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:34:10 minikube kubelet[1371]: I1021 13:34:10.655527    1371 scope.go:117] "RemoveContainer" containerID="1ae7de50cc89cef1c6c92126fc108e0bf3018d3e804febdc68dccb2c1c1b9f69"
Oct 21 13:34:11 minikube kubelet[1371]: I1021 13:34:11.452395    1371 scope.go:117] "RemoveContainer" containerID="1ae7de50cc89cef1c6c92126fc108e0bf3018d3e804febdc68dccb2c1c1b9f69"
Oct 21 13:34:11 minikube kubelet[1371]: I1021 13:34:11.452585    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:34:11 minikube kubelet[1371]: E1021 13:34:11.452657    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:34:23 minikube kubelet[1371]: I1021 13:34:23.657894    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:34:23 minikube kubelet[1371]: E1021 13:34:23.659987    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:34:37 minikube kubelet[1371]: I1021 13:34:37.656231    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:34:37 minikube kubelet[1371]: E1021 13:34:37.656716    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:34:48 minikube kubelet[1371]: I1021 13:34:48.657014    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:34:48 minikube kubelet[1371]: E1021 13:34:48.657533    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:35:02 minikube kubelet[1371]: I1021 13:35:02.656737    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:35:03 minikube kubelet[1371]: I1021 13:35:03.343404    1371 scope.go:117] "RemoveContainer" containerID="ed5e96ac2c56677dd2d1a36f6050495f73fa1e4b823793d2f00ac3ab82fa7966"
Oct 21 13:35:03 minikube kubelet[1371]: I1021 13:35:03.343516    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:35:03 minikube kubelet[1371]: E1021 13:35:03.343583    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:35:16 minikube kubelet[1371]: I1021 13:35:16.656596    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:35:16 minikube kubelet[1371]: E1021 13:35:16.656926    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:35:27 minikube kubelet[1371]: I1021 13:35:27.655659    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:35:27 minikube kubelet[1371]: E1021 13:35:27.656136    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:35:42 minikube kubelet[1371]: I1021 13:35:42.657148    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:35:42 minikube kubelet[1371]: E1021 13:35:42.657603    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:35:57 minikube kubelet[1371]: I1021 13:35:57.655101    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:35:57 minikube kubelet[1371]: E1021 13:35:57.655244    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:36:12 minikube kubelet[1371]: I1021 13:36:12.658074    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:36:12 minikube kubelet[1371]: E1021 13:36:12.658508    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:36:25 minikube kubelet[1371]: I1021 13:36:25.655389    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:36:26 minikube kubelet[1371]: I1021 13:36:26.777377    1371 scope.go:117] "RemoveContainer" containerID="5cf7a2d955d16fa6e274d7ee3115979aaf57c882e290476c1d63b09775343b73"
Oct 21 13:36:26 minikube kubelet[1371]: I1021 13:36:26.777577    1371 scope.go:117] "RemoveContainer" containerID="a0c3dbb876f9b7c3b66654a7c8bdd0866a73daf8068cd0e662e3606104eb6267"
Oct 21 13:36:26 minikube kubelet[1371]: E1021 13:36:26.777680    1371 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"worker-app\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=worker-app pod=worker-app-pod_default(b7956bf8-d67d-4435-81ca-667f0c8afb00)\"" pod="default/worker-app-pod" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00"
Oct 21 13:36:35 minikube kubelet[1371]: I1021 13:36:35.530181    1371 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wvxgf\" (UniqueName: \"kubernetes.io/projected/b7956bf8-d67d-4435-81ca-667f0c8afb00-kube-api-access-wvxgf\") pod \"b7956bf8-d67d-4435-81ca-667f0c8afb00\" (UID: \"b7956bf8-d67d-4435-81ca-667f0c8afb00\") "
Oct 21 13:36:35 minikube kubelet[1371]: I1021 13:36:35.532839    1371 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/b7956bf8-d67d-4435-81ca-667f0c8afb00-kube-api-access-wvxgf" (OuterVolumeSpecName: "kube-api-access-wvxgf") pod "b7956bf8-d67d-4435-81ca-667f0c8afb00" (UID: "b7956bf8-d67d-4435-81ca-667f0c8afb00"). InnerVolumeSpecName "kube-api-access-wvxgf". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Oct 21 13:36:35 minikube kubelet[1371]: I1021 13:36:35.631078    1371 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-wvxgf\" (UniqueName: \"kubernetes.io/projected/b7956bf8-d67d-4435-81ca-667f0c8afb00-kube-api-access-wvxgf\") on node \"minikube\" DevicePath \"\""
Oct 21 13:36:35 minikube kubelet[1371]: I1021 13:36:35.959782    1371 scope.go:117] "RemoveContainer" containerID="a0c3dbb876f9b7c3b66654a7c8bdd0866a73daf8068cd0e662e3606104eb6267"
Oct 21 13:36:36 minikube kubelet[1371]: I1021 13:36:36.668502    1371 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="b7956bf8-d67d-4435-81ca-667f0c8afb00" path="/var/lib/kubelet/pods/b7956bf8-d67d-4435-81ca-667f0c8afb00/volumes"
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.789729    1371 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"postgres-storage\" (UniqueName: \"kubernetes.io/empty-dir/d725fa83-66a2-40d4-8b2b-63e87998fa92-postgres-storage\") pod \"d725fa83-66a2-40d4-8b2b-63e87998fa92\" (UID: \"d725fa83-66a2-40d4-8b2b-63e87998fa92\") "
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.789806    1371 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-2xq6v\" (UniqueName: \"kubernetes.io/projected/d725fa83-66a2-40d4-8b2b-63e87998fa92-kube-api-access-2xq6v\") pod \"d725fa83-66a2-40d4-8b2b-63e87998fa92\" (UID: \"d725fa83-66a2-40d4-8b2b-63e87998fa92\") "
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.792867    1371 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d725fa83-66a2-40d4-8b2b-63e87998fa92-kube-api-access-2xq6v" (OuterVolumeSpecName: "kube-api-access-2xq6v") pod "d725fa83-66a2-40d4-8b2b-63e87998fa92" (UID: "d725fa83-66a2-40d4-8b2b-63e87998fa92"). InnerVolumeSpecName "kube-api-access-2xq6v". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.814691    1371 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/empty-dir/d725fa83-66a2-40d4-8b2b-63e87998fa92-postgres-storage" (OuterVolumeSpecName: "postgres-storage") pod "d725fa83-66a2-40d4-8b2b-63e87998fa92" (UID: "d725fa83-66a2-40d4-8b2b-63e87998fa92"). InnerVolumeSpecName "postgres-storage". PluginName "kubernetes.io/empty-dir", VolumeGIDValue ""
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.895702    1371 reconciler_common.go:299] "Volume detached for volume \"postgres-storage\" (UniqueName: \"kubernetes.io/empty-dir/d725fa83-66a2-40d4-8b2b-63e87998fa92-postgres-storage\") on node \"minikube\" DevicePath \"\""
Oct 21 13:37:04 minikube kubelet[1371]: I1021 13:37:04.895783    1371 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-2xq6v\" (UniqueName: \"kubernetes.io/projected/d725fa83-66a2-40d4-8b2b-63e87998fa92-kube-api-access-2xq6v\") on node \"minikube\" DevicePath \"\""
Oct 21 13:37:05 minikube kubelet[1371]: I1021 13:37:05.559148    1371 scope.go:117] "RemoveContainer" containerID="07868bb517c89a1d8974b77c9976a1dc232d03fc817b6b52cd007c91f12f7688"
Oct 21 13:37:06 minikube kubelet[1371]: I1021 13:37:06.667005    1371 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="d725fa83-66a2-40d4-8b2b-63e87998fa92" path="/var/lib/kubelet/pods/d725fa83-66a2-40d4-8b2b-63e87998fa92/volumes"
Oct 21 13:37:14 minikube kubelet[1371]: I1021 13:37:14.277206    1371 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hkh27\" (UniqueName: \"kubernetes.io/projected/a15277e7-88f0-4f68-8527-baa2d6fd1238-kube-api-access-hkh27\") pod \"postgres-pod\" (UID: \"a15277e7-88f0-4f68-8527-baa2d6fd1238\") " pod="default/postgres-pod"
Oct 21 13:37:14 minikube kubelet[1371]: I1021 13:37:14.277391    1371 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"postgres-storage\" (UniqueName: \"kubernetes.io/empty-dir/a15277e7-88f0-4f68-8527-baa2d6fd1238-postgres-storage\") pod \"postgres-pod\" (UID: \"a15277e7-88f0-4f68-8527-baa2d6fd1238\") " pod="default/postgres-pod"
Oct 21 13:37:14 minikube kubelet[1371]: I1021 13:37:14.707959    1371 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/postgres-pod" podStartSLOduration=0.707930211 podStartE2EDuration="707.930211ms" podCreationTimestamp="2025-10-21 13:37:14 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-21 13:37:14.707792336 +0000 UTC m=+42948.170364236" watchObservedRunningTime="2025-10-21 13:37:14.707930211 +0000 UTC m=+42948.170502111"
Oct 21 13:37:59 minikube kubelet[1371]: I1021 13:37:59.492137    1371 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ttb6k\" (UniqueName: \"kubernetes.io/projected/c85a99b0-af10-468a-94fe-8bc058be7a77-kube-api-access-ttb6k\") pod \"worker-app-pod\" (UID: \"c85a99b0-af10-468a-94fe-8bc058be7a77\") " pod="default/worker-app-pod"
Oct 21 13:38:00 minikube kubelet[1371]: I1021 13:38:00.469330    1371 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/worker-app-pod" podStartSLOduration=1.469317968 podStartE2EDuration="1.469317968s" podCreationTimestamp="2025-10-21 13:37:59 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-10-21 13:38:00.469232843 +0000 UTC m=+42993.931844673" watchObservedRunningTime="2025-10-21 13:38:00.469317968 +0000 UTC m=+42993.931929798"


==> storage-provisioner [91a87e17c550] <==
I1016 10:33:47.470943       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1016 10:34:17.542520       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [f3fa24584146] <==
W1021 13:45:32.716501       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:32.722814       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:34.728779       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:34.734392       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:36.741471       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:36.746409       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:38.754760       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:38.761364       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:40.767454       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:40.772722       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:42.786302       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:42.796364       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:44.806134       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:44.813762       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:46.820911       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:46.828772       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:48.835103       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:48.843794       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:50.849769       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:50.855725       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:52.861276       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:52.866587       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:54.871998       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:54.877098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:56.883783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:56.889455       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:58.894044       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:45:58.898864       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:00.903407       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:00.906867       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:02.912241       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:02.919313       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:04.922898       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:04.928719       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:06.939769       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:06.951767       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:08.955425       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:08.958411       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:10.962434       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:10.969574       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:12.977440       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:12.984871       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:14.990538       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:14.995200       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:17.003058       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:17.010098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:19.016772       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:19.026139       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:21.032504       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:21.037906       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:23.042206       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:23.046551       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:25.051624       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:25.056664       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:27.060910       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:27.069319       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:29.073060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:29.076682       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:31.080068       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1021 13:46:31.081863       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

